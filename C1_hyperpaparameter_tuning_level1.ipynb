{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "Tdag9uozvvLz",
    "outputId": "61c39a27-a703-431d-da81-a10cfb5b8d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (0.21)\n",
      "Requirement already satisfied: scipy in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (1.4.1)\n",
      "Requirement already satisfied: plotly in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (4.5.4)\n",
      "Requirement already satisfied: matplotlib in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (3.1.3)\n",
      "Requirement already satisfied: graphviz in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (0.13.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (1.0.1)\n",
      "Requirement already satisfied: six in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from catboost) (1.18.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->catboost) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->catboost) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from pandas>=0.24.0->catboost) (2019.3)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "-SPiv6C7jZ0_",
    "outputId": "ba89e2ef-1387-4a79-9513-602589d5a680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: scipy>=0.18.0 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-optimize) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-optimize) (0.22.1)\n",
      "Requirement already satisfied: pyaml>=16.9 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-optimize) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from scikit-optimize) (0.14.1)\n",
      "Requirement already satisfied: PyYAML in e:\\anaconda3\\envs\\tf-gpu\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87nmhuBOxwkZ"
   },
   "source": [
    "Now that we have fixed and generated three feature subsets\n",
    "1. non-lagged + lagged textual features\n",
    "2. lagged {target,item,shop} + non-lagged basic categories\n",
    "3. lagged features within shop\n",
    "\n",
    "and three first level classifiers types for each\n",
    "* a.  CatBoost\n",
    "* b. RidgeCV \n",
    "* c. Random Forest (sklearn) \n",
    "\n",
    "we search for hyperarameters that are used for predicting a month \n",
    "based on twelve month history, with one month gap between training and prediction periods.\n",
    "\n",
    "This is a compromise of the prediction quality on the other hand, and not having the prediction \n",
    "quality and optimal hyperparameters vary too much over the training period when generating the first level predictions as input features of second stacking level.\n",
    "\n",
    "The search for hyperparameters is problematic in whole because the chosen validation scheme is lacking. There may not be\n",
    "too much that can be done, because the validation data necessarily has different distribution as the actual testing data.\n",
    "This is because the temporal nature of the prediction problem. The distributions slowly drift during cause of time. Therefore, \n",
    "it is good to have the validation period temporally close to the test period. On the other hand, data analysis shows strong seasonal=(yearly) effects. \n",
    "Predicting October sales based on previous year simply is a very different problem to predicting December sales, as sales figures seem to peak strongly in December and have special characteristics.\n",
    "\n",
    "We decide to search for such hyperparameters that maximise the quality of predictions (with\n",
    "reasonable computational burden) in the hold-out validation data of Oct 2015. This is despite the fact that we have seen in examples that\n",
    "such optimal model hyperparameters do not result in optimal prediction quality for Dec 2015.\n",
    "We specifically do not search for such hyperparameters (via a coross-validation scheme) that would maximise the quality of predictions during\n",
    "the training period, as the value of temporally distant predictions is questionable after because of the distribution shift throughtime.\n",
    "\n",
    "\n",
    "The parameters are used for\n",
    "a) creating submissions for ensembling using simple schemes\n",
    "b) generating level 2 input features for a stacking algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Q3ZlcjWEw6ej",
    "outputId": "463d1d44-e14c-498d-a40e-98e5b787f240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR  e:\\repos\\kaggle-c1\\competitive-data-science-predict-future-sales\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "from itertools import product\n",
    "import gc\n",
    "#from tqdm import tqdm_notebook\n",
    "import re\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from time import time\n",
    "import pprint\n",
    "\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive') \n",
    "  if not os.path.isfile('SETTINGS.json'):\n",
    "       # hard coded data directory in drive is used if SETTINGS.json not present \n",
    "       config={}\n",
    "       config['DATA_DIR'] = '/content/gdrive/My Drive/kaggle-c1'\n",
    "       with open('SETTINGS.json', 'w') as outfile:\n",
    "         json.dump(config, outfile)\n",
    "\n",
    "with open('SETTINGS.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "DATA_DIR = config['DATA_DIR']\n",
    "\n",
    "print('Using DATA_DIR ', DATA_DIR)\n",
    "\n",
    "DATA_FOLDER = DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5LC3ULfxHOu"
   },
   "outputs": [],
   "source": [
    "test_spec = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\n",
    "\n",
    "index_cols=['item_id','shop_id','date_block_num']\n",
    "date_block_val = 33\n",
    "date_block_test = 35 # Dec 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214195</th>\n",
       "      <td>214195</td>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214196</th>\n",
       "      <td>214196</td>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214197</th>\n",
       "      <td>214197</td>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214198</th>\n",
       "      <td>214198</td>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214199</th>\n",
       "      <td>214199</td>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  shop_id  item_id\n",
       "0            0        5     5037\n",
       "1            1        5     5320\n",
       "2            2        5     5233\n",
       "3            3        5     5232\n",
       "4            4        5     5268\n",
       "...        ...      ...      ...\n",
       "214195  214195       45    18454\n",
       "214196  214196       45    16188\n",
       "214197  214197       45    15757\n",
       "214198  214198       45    19648\n",
       "214199  214199       45      969\n",
       "\n",
       "[214200 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5ojiapTk4Lk"
   },
   "outputs": [],
   "source": [
    "# a wrapper class to use pre-defined division to training and hold-out set\n",
    "# as a cross-validation object\n",
    "\n",
    "class HoldOut:\n",
    "    \"\"\"\n",
    "    Hold-out cross-validator generator. In the hold-out, the\n",
    "    data is split only once into a train set and a test set.\n",
    "    Here the split is given as a input parameter in the class initialisation\n",
    "    Unlike in other cross-validation schemes, the hold-out\n",
    "    consists of only one iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_indices, test_indices : the class just passes on these when yielding splits\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_indices, test_indices):\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self.train_indices, self.test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDotqGtiygOd"
   },
   "source": [
    "Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMupO1roxwke"
   },
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCDUSYw8rWZX"
   },
   "outputs": [],
   "source": [
    "def write_predictions_by_array(array, filename):\n",
    "  df=pd.DataFrame(array)\n",
    "  df.columns=['item_cnt_month']\n",
    "  df.to_csv(os.path.join(DATA_FOLDER, filename), index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuA0NhdkyboL"
   },
   "outputs": [],
   "source": [
    "def clipped_rmse(gt, predicted,clip_min=0, clip_max=20):\n",
    "  target=np.minimum(np.maximum(gt,clip_min), clip_max)\n",
    "  return np.sqrt((target-predicted)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeoY-KwwjZ1c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fs2wwaaVP3G"
   },
   "outputs": [],
   "source": [
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    d=pd.DataFrame(optimizer.cv_results_)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUAMRMWqZFz7"
   },
   "outputs": [],
   "source": [
    "def find_catboost_parameters_bayes(feature_file_name, search_space=None, n_iter=100):\n",
    "    all_data = pd.read_csv(os.path.join(DATA_FOLDER, feature_file_name))\n",
    "\n",
    "    dates=all_data['date_block_num']\n",
    "\n",
    "    dates_train = (dates>= date_block_val - 13) & (dates<= date_block_val - 2)\n",
    "    dates_trainval = (dates>= date_block_test - 13) & (dates<= date_block_test - 2)\n",
    "\n",
    "# extract training, validation and test sets (labels and features)\n",
    "\n",
    "    y_train=all_data.loc[dates_train, 'target']\n",
    "    y_trainval=all_data.loc[dates_trainval, 'target']\n",
    "    y_val = all_data.loc[dates == date_block_val, 'target']\n",
    "\n",
    "    to_drop_cols = ['target','date_block_num']\n",
    "\n",
    "    X_train = all_data.loc[dates_train].drop(to_drop_cols, axis=1)\n",
    "    X_trainval = all_data.loc[dates_trainval].drop(to_drop_cols, axis=1)\n",
    "    X_val = all_data.loc[dates == date_block_val].drop(to_drop_cols, axis=1)\n",
    "    X_test = all_data.loc[dates == date_block_test].drop(to_drop_cols, axis=1)\n",
    "\n",
    "    # determine how to permute test set predictions for submission generation \n",
    "\n",
    "    shop_item2submissionid={}\n",
    "    for idx, row in test_spec.iterrows():\n",
    "        shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])] = row['ID']\n",
    "\n",
    "    test_data=all_data.loc[dates == date_block_test, ['shop_id','item_id']]    \n",
    "\n",
    "    testidx2submissionidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "    for idx in range(test_data.shape[0]):\n",
    "        row =test_data.iloc[idx]\n",
    "        testidx2submissionidx[idx] = shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])]\n",
    "\n",
    "    #invert the mapping\n",
    "    submissionidx2testidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "    for i in range(test_data.shape[0]):\n",
    "        submissionidx2testidx[testidx2submissionidx[i]]=i\n",
    "\n",
    "    del test_data\n",
    "    gc.collect()    \n",
    "\n",
    "    X_paramsearch =  pd.concat([X_train, X_val],ignore_index=True)\n",
    "    y_paramsearch = pd.concat([y_train, y_val],ignore_index=True)\n",
    "    \n",
    "    train_indices = np.arange(X_train.shape[0])\n",
    "    val_indices = np.arange(X_val.shape[0]) + X_train.shape[0]\n",
    "    \n",
    "    mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    if search_space is None: \n",
    "        search_space = {'iterations': Integer(300, 1500),\n",
    "                        'depth': Integer(8, 16),\n",
    "                        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "                        'random_strength': Real(1, 100, 'log-uniform'),\n",
    "                        'l2_leaf_reg': Real(0.001, 2.0, 'log-uniform'),\n",
    "                        }\n",
    "\n",
    "    clf = CatBoostRegressor(task_type='GPU',has_time=True, verbose=False)\n",
    "\n",
    "\n",
    "    # Setting up BayesSearchCV\n",
    "\n",
    "    cv = HoldOut(train_indices=train_indices, test_indices=val_indices)\n",
    "\n",
    "    opt = BayesSearchCV(clf,\n",
    "                        search_space,\n",
    "                        scoring=mse_scorer,\n",
    "                        cv=cv,\n",
    "                        n_iter=n_iter,\n",
    "                        n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n",
    "                        return_train_score=False,\n",
    "                        refit=False,\n",
    "                        optimizer_kwargs={'base_estimator': 'GP'})\n",
    "\n",
    "    best_params = report_perf(opt, X_paramsearch, y_paramsearch,'CatBoost', \n",
    "                               callbacks=[VerboseCallback(100)])\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhGMug07xwkn"
   },
   "source": [
    "# Feature set 1: non-lagged and lagged basic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "em4Q8my4jZ1k"
   },
   "outputs": [],
   "source": [
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv')\n",
    "#print(best_params_basic)\n",
    "# CatBoost took 5376.07 seconds,  candidates checked: 100, best CV score: -20.311 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 9),\n",
    "#             ('iterations', 339),\n",
    "#             ('l2_leaf_reg', 0.1043327165183886),\n",
    "#             ('learning_rate', 0.03646613151300171),\n",
    "#             ('random_strength', 70.08445579850765)])\n",
    "#\n",
    "# OrderedDict([('depth', 9), ('iterations', 339), ('l2_leaf_reg', 0.1043327165183886), ('learning_rate', 0.03646613151300171), ('random_strength', 70.08445579850765)])\n",
    "\n",
    "#CatBoost took 6637.58 seconds,  candidates checked: 100, best CV score: -19.772 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 300),\n",
    "#             ('l2_leaf_reg', 2.0),\n",
    "#             ('learning_rate', 0.08922738789933286),\n",
    "#             ('random_strength', 100.0)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 300), ('l2_leaf_reg', 2.0), ('learning_rate', 0.08922738789933286), ('random_strength', 100.0)])\n",
    "\n",
    "#CatBoost took 4328.61 seconds,  candidates checked: 100, best CV score: -17.769 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 100.0)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 1500), ('l2_leaf_reg', 0.001), ('learning_rate', 0.1813605946638354), ('random_strength', 100.0)])\n",
    "#CatBoost took 5568.42 seconds,  candidates checked: 100, best CV score: -17.705 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 300),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.21121409098653413),\n",
    "#             ('random_strength', 100.0)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 300), ('l2_leaf_reg', 0.001), ('learning_rate', 0.21121409098653413), ('random_strength', 100.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "hBr7M8dF2fM5",
    "outputId": "71d0c12c-a171-4091-e9a1-ac4a98f88fff"
   },
   "outputs": [],
   "source": [
    "# from the best point found, start line searches wrt. one parameter at time\n",
    "# start from ones that reach the limit (=random strength, l2_leaf_reg)\n",
    "#search_space = {'iterations': Categorical([1500]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.1813605946638354]),\n",
    "#                'random_strength': Real(80,3000,'log-uniform'),\n",
    "#                'l2_leaf_reg': Categorical([0.001]),\n",
    "#                }\n",
    "\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 1424.11 seconds,  candidates checked: 20, best CV score: -18.071 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 80.0)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 1500), ('l2_leaf_reg', 0.001), ('learning_rate', 0.1813605946638354), ('random_strength', 80.0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWsWuUNJMh3K"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Categorical([1500]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.1813605946638354]),\n",
    "#                'random_strength': Real(1,80,'log-uniform'),\n",
    "#                'l2_leaf_reg': Categorical([0.001]),\n",
    "#                }\n",
    "\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 1431.23 seconds,  candidates checked: 20, best CV score: -18.482 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 79.99999999999999)])\n",
    "\n",
    "#OrderedDict([('depth', 8), ('iterations', 1500), ('l2_leaf_reg', 0.001), ('learning_rate', 0.1813605946638354), ('random_strength', 79.99999999999999)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVwANFajTZvS"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Categorical([1500]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.1813605946638354]),\n",
    "#                'random_strength': Categorical([80]),\n",
    "#                'l2_leaf_reg': Real(0.00001,0.1,'log-uniform'),\n",
    "#                }#\n",
    "#\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 1426.77 seconds,  candidates checked: 20, best CV score: -17.929 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 80)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 1500), ('l2_leaf_reg', 0.005744167167485476), ('learning_rate', 0.1813605946638354), ('random_strength', 80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTeDZX5uWcH4"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Integer(100,2000,'log-uniform'),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.1813605946638354]),\n",
    "#                'random_strength': Categorical([80]),\n",
    "#                'l2_leaf_reg': Categorical([ 0.005744167167485476]),\n",
    "#                }\n",
    "#\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 901.52 seconds,  candidates checked: 20, best CV score: -17.552 ± 0.000\n",
    "#Best parameters:\n",
    "#rderedDict([('depth', 8),\n",
    "#             ('iterations', 107),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 80)])\n",
    "\n",
    "# OrderedDict([('depth', 8), ('iterations', 107), ('l2_leaf_reg', 0.005744167167485476), ('learning_rate', 0.1813605946638354), ('random_strength', 80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wwm3uwyvWcGn"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Categorical([107]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Real(0.1,0.3,'uniform'),\n",
    "#                'random_strength': Categorical([80]),\n",
    "#                'l2_leaf_reg': Categorical([ 0.005744167167485476]),\n",
    "#                }\n",
    "#\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "#\n",
    "#Iteration No: 21 started. Searching for the next optimal point.\n",
    "#CatBoost took 146.82 seconds,  candidates checked: 20, best CV score: -17.566 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 107),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.23068135624313127),\n",
    "#             ('random_strength', 80)])\n",
    "#\n",
    "#OrderedDict([('depth', 8), ('iterations', 107), ('l2_leaf_reg', 0.005744167167485476), ('learning_rate', 0.23068135624313127), ('random_strength', 80)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joHb1WnHg98T"
   },
   "source": [
    "Experiment with some additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yavm1Ni1hHlI"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Categorical([107]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.181]),\n",
    "#                'random_strength': Categorical([80]),\n",
    "#                'l2_leaf_reg': Categorical([ 0.005744167167485476]),\n",
    "#                'bagging_temperature': Real(0.01,100,'log-uniform')\n",
    "#                }\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 141.68 seconds,  candidates checked: 20, best CV score: -17.738 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('bagging_temperature', 0.9784460719042715),\n",
    "#             ('depth', 8),\n",
    "#             ('iterations', 107),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.181),\n",
    "#            ('random_strength', 80)])\n",
    "# OrderedDict([('bagging_temperature', 0.9784460719042715), ('depth', 8), ('iterations', 107), ('l2_leaf_reg', 0.005744167167485476), ('learning_rate', 0.181), ('random_strength', 80)])\n",
    "\n",
    "# no improvement from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qb1tkjE65oL3"
   },
   "outputs": [],
   "source": [
    "#search_space = {'iterations': Categorical([107]),   \n",
    "#                'depth': Categorical([8]), \n",
    "#                'learning_rate': Categorical([0.181]),\n",
    "#                'random_strength': Categorical([80]),\n",
    "#                'l2_leaf_reg': Categorical([ 0.005744167167485476]),\n",
    "#                'grow_policy': Categorical(['SymmetricTree','Depthwise','Lossguide'])\n",
    "#                }\n",
    "#best_params_basic = find_catboost_parameters_bayes('feature_set_basic.csv', search_space=search_space, n_iter=20)\n",
    "#print(best_params_basic)\n",
    "\n",
    "#CatBoost took 195.46 seconds,  candidates checked: 20, best CV score: -19.665 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('grow_policy', 'Lossguide'),\n",
    "#             ('iterations', 107),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.181),\n",
    "#             ('random_strength', 80)])\n",
    "\n",
    "#OrderedDict([('depth', 8), ('grow_policy', 'Lossguide'), ('iterations', 107), ('l2_leaf_reg', 0.005744167167485476), ('learning_rate', 0.181), ('random_strength', 80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OH0F7t_f5oKd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDHfLMgijZ1m"
   },
   "outputs": [],
   "source": [
    "#best_params_text = find_catboost_parameters_bayes('feature_set_text.csv')\n",
    "#print(best_params_text)\n",
    "\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 16),\n",
    "#             ('iterations', 300),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.17754758868410853),\n",
    "#             ('random_strength', 1.0)])\n",
    "#\n",
    "#OrderedDict([('depth', 16), ('iterations', 300), ('l2_leaf_reg', 0.001), ('learning_rate', 0.17754758868410853), ('random_strength', 1.0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHjOogYwjZ1p"
   },
   "outputs": [],
   "source": [
    "#best_params_within = find_catboost_parameters_bayes('feature_set_within.csv')\n",
    "#print(best_params_within)\n",
    "#CatBoost took 4357.06 seconds,  candidates checked: 100, best CV score: -21.848 ± 0.000\n",
    "#Best parameters:\n",
    "#OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 2.0),\n",
    "#             ('learning_rate', 0.058681717789493895),\n",
    "#             ('random_strength', 2.168205069150344)])\n",
    "#\n",
    "# OrderedDict([('depth', 8), ('iterations', 1500), ('l2_leaf_reg', 2.0), ('learning_rate', 0.058681717789493895), ('random_strength', 2.168205069150344)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHn4StEBZI5J"
   },
   "outputs": [],
   "source": [
    "#to_drop_cols=[] # [col for col in X_train.columns.values if re.search('internet',col)]\n",
    "#reg=CatBoostRegressor(task_type='GPU', iterations=100, eta=0.3,depth=10, metric_period=20)\n",
    "#reg.fit(X_trainval.drop(to_drop_cols,axis=1).to_numpy(), y_trainval)\n",
    "#pred_test = np.clip(reg.predict(X_test.drop(to_drop_cols,axis=1).to_numpy()), 0, 20)\n",
    "#write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-catboost-feature_set_basic.csv')\n",
    "\n",
    "# LB  1.008784 and 1.027125 (d=8, iterations=100, eta=0.3)\n",
    "# 1.046352 and 1.054449 (d=10, iterations=100, eta=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final parameter search step, try out the best parameters found for each of the three feature sets to all of the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic features\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(DATA_FOLDER, 'feature_set_basic.csv'))\n",
    "\n",
    "dates=all_data['date_block_num']\n",
    "\n",
    "dates_train = (dates>= date_block_val - 13) & (dates<= date_block_val - 2)\n",
    "dates_trainval = (dates>= date_block_test - 13) & (dates<= date_block_test - 2)\n",
    "# y_train = all_data.loc[(dates>= date_block_val - 13) & (dates<= date_block_val - 2), 'target']\n",
    "# y_trainval = all_data.loc[(dates>= date_block_test - 13) & (dates<= date_block_test - 2), 'target']\n",
    "\n",
    "y_train=all_data.loc[dates_train, 'target']\n",
    "#y_trainval=all_data.loc[dates_trainval, 'target']\n",
    "y_val = all_data.loc[dates == date_block_val, 'target']\n",
    "#y_test = all_data.loc[dates == date_block_test, 'target']\n",
    "\n",
    "to_drop_cols = ['target','date_block_num']\n",
    "\n",
    "X_train = all_data.loc[dates_train].drop(to_drop_cols, axis=1)\n",
    "#X_trainval = all_data.loc[dates_trainval].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[dates == date_block_val].drop(to_drop_cols, axis=1)\n",
    "#X_test = all_data.loc[dates == date_block_test].drop(to_drop_cols, axis=1)\n",
    "\n",
    "gc.collect()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters for basic features by Bayesian search:\n",
    "#rderedDict([('depth', 8),\n",
    "#             ('iterations', 107),\n",
    "#             ('l2_leaf_reg', 0.005744167167485476),\n",
    "#             ('learning_rate', 0.1813605946638354),\n",
    "#             ('random_strength', 80)])\n",
    "\n",
    "#Best parameters for text features found in Bayesian search:\n",
    "#OrderedDict([('depth', 16),\n",
    "#             ('iterations', 300),\n",
    "#             ('l2_leaf_reg', 0.001),\n",
    "#             ('learning_rate', 0.17754758868410853),\n",
    "#             ('random_strength', 1.0)])\n",
    "\n",
    "#Best parameters for within-shop features found in Bayesian search:\n",
    "# OrderedDict([('depth', 8),\n",
    "#             ('iterations', 1500),\n",
    "#             ('l2_leaf_reg', 2.0),\n",
    "#             ('learning_rate', 0.058681717789493895),\n",
    "#             ('random_strength', 2.168205069150344)])\n",
    "\n",
    "\n",
    "\n",
    "reg=CatBoostRegressor(task_type='GPU', depth=8, iterations=107, l2_leaf_reg=0.005744167167485476,\n",
    "                     learning_rate=0.1813605946638354, random_strength=80, verbose=10)\n",
    "reg.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "print('Clipped RMSE {} for parameters meant for basic features'.format(clipped_rmse(y_val, pred_val)))\n",
    "# Clipped RMSE 0.3787635987985689 for parameters meant for basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=CatBoostRegressor(task_type='GPU', depth=16, iterations=300, l2_leaf_reg=0.001,\n",
    "                     learning_rate=0.17754758868410853, random_strength=1, verbose=10)\n",
    "reg.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "print('Clipped RMSE {} for parameters meant for text features'.format(clipped_rmse(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.4648913\ttotal: 53.7ms\tremaining: 5.7s\n",
      "10:\tlearn: 2.6844193\ttotal: 609ms\tremaining: 5.32s\n",
      "20:\tlearn: 2.5467547\ttotal: 1.19s\tremaining: 4.88s\n",
      "30:\tlearn: 2.4088414\ttotal: 1.78s\tremaining: 4.38s\n",
      "40:\tlearn: 2.3470181\ttotal: 2.37s\tremaining: 3.81s\n",
      "50:\tlearn: 2.2864916\ttotal: 2.96s\tremaining: 3.25s\n",
      "60:\tlearn: 2.2538157\ttotal: 3.55s\tremaining: 2.68s\n",
      "70:\tlearn: 2.2053329\ttotal: 4.16s\tremaining: 2.11s\n",
      "80:\tlearn: 2.1775419\ttotal: 4.75s\tremaining: 1.52s\n",
      "90:\tlearn: 2.1609599\ttotal: 5.35s\tremaining: 940ms\n",
      "100:\tlearn: 2.1378800\ttotal: 5.93s\tremaining: 352ms\n",
      "106:\tlearn: 2.0901688\ttotal: 6.28s\tremaining: 0us\n",
      "Clipped RMSE 0.3787635987985689 for parameters meant for basic features\n"
     ]
    }
   ],
   "source": [
    "reg=CatBoostRegressor(task_type='GPU', depth=8, iterations=1500, l2_leaf_reg=2,\n",
    "                     learning_rate=0.058681717789493895, random_strength=2.168205069150344, verbose=10)\n",
    "reg.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "print('Clipped RMSE {} for parameters meant for within-shop features'.format(clipped_rmse(y_val, pred_val)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "C1-hyperpaparameter-tuning-level1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
