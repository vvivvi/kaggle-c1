{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fixed and generated three feature subsets\n",
    "1. non-lagged + lagged textual features\n",
    "2. lagged {target,item,shop} + non-lagged basic categories\n",
    "3. lagged features within shop\n",
    "\n",
    "and three first level classifiers types for each\n",
    "* a.  CatBoost\n",
    "* b. RidgeCV \n",
    "* c. Random Forest (sklearn) \n",
    "\n",
    "we search for hyperarameters that are used for predicting a month \n",
    "based on twelve month history, with one month gap between training and prediction periods.\n",
    "\n",
    "This is a compromise of the prediction quality on the other hand, and not having the prediction \n",
    "quality and optimal hyperparameters vary too much over the training period when generating the first level predictions as input features of second stacking level.\n",
    "\n",
    "The search for hyperparameters is problematic in whole because the chosen validation scheme is lacking. There may not be\n",
    "too much that can be done, because the validation data necessarily has different distribution as the actual testing data.\n",
    "This is because the temporal nature of the prediction problem. The distributions slowly drift during cause of time. Therefore, \n",
    "it is good to have the validation period temporally close to the test period. On the other hand, data analysis shows strong seasonal=(yearly) effects. \n",
    "Predicting October sales based on previous year simply is a very different problem to predicting December sales, as sales figures seem to peak strongly in December and have special characteristics.\n",
    "\n",
    "We decide to search for such hyperparameters that maximise the quality of predictions (with\n",
    "reasonable computational burden) in the hold-out validation data of Oct 2015. This is despite the fact that we have seen in examples that\n",
    "such optimal model hyperparameters do not result in optimal prediction quality for Dec 2015.\n",
    "We specifically do not search for such hyperparameters (via a coross-validation scheme) that would maximise the quality of predictions during\n",
    "the training period, as the value of temporally distant predictions is questionable after because of the distribution shift throughtime.\n",
    "\n",
    "\n",
    "The parameters are used for\n",
    "a) creating submissions for ensembling using simple schemes\n",
    "b) generating level 2 input features for a stacking algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CY0RZpd88blS",
    "outputId": "eeb875e8-6911-4c03-f3ef-1384b94be160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.18.1\n",
      "pandas 0.25.3\n",
      "scipy 1.4.1\n",
      "sklearn 0.22.1\n",
      "lightgbm 2.3.1\n",
      "catboost 0.22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "import catboost\n",
    "import gc\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lgb, catboost]:\n",
    "    print (p.__name__, p.__version__)\n",
    "    \n",
    "DATA_FOLDER = 'competitive-data-science-predict-future-sales'\n",
    "test_spec = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\n",
    "\n",
    "index_cols=['item_id','shop_id','date_block_num']\n",
    "date_block_val = 33\n",
    "date_block_test = 35 # Dec 2015\n",
    "\n",
    "test2submission_mapping_generated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCDUSYw8rWZX"
   },
   "outputs": [],
   "source": [
    "def write_predictions_by_array(array, filename):\n",
    "  df=pd.DataFrame(array)\n",
    "  df.columns=['item_cnt_month']\n",
    "  df.to_csv(os.path.join(DATA_FOLDER, filename), index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF4l3wSvb020"
   },
   "outputs": [],
   "source": [
    "def clipped_rmse(gt, predicted,clip_min=0, clip_max=20):\n",
    "  target=np.minimum(np.maximum(gt,clip_min), clip_max)\n",
    "  return np.sqrt((target-predicted)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature set 1: non-lagged and lagged basic categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(DATA_FOLDER, 'feature_set_basic.csv'))\n",
    "\n",
    "dates=all_data['date_block_num']\n",
    "\n",
    "dates_train = (dates>= date_block_val - 13) & (dates<= date_block_val - 2)\n",
    "dates_trainval = (dates>= date_block_test - 13) & (dates<= date_block_test - 2)\n",
    "# y_train = all_data.loc[(dates>= date_block_val - 13) & (dates<= date_block_val - 2), 'target']\n",
    "# y_trainval = all_data.loc[(dates>= date_block_test - 13) & (dates<= date_block_test - 2), 'target']\n",
    "\n",
    "y_train=all_data.loc[dates_train, 'target']\n",
    "y_trainval=all_data.loc[dates_trainval, 'target']\n",
    "y_val = all_data.loc[dates == date_block_val, 'target']\n",
    "y_test = all_data.loc[dates == date_block_test, 'target']\n",
    "\n",
    "to_drop_cols = ['target','date_block_num']\n",
    "\n",
    "X_train = all_data.loc[dates_train].drop(to_drop_cols, axis=1)\n",
    "X_trainval = all_data.loc[dates_trainval].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[dates == date_block_val].drop(to_drop_cols, axis=1)\n",
    "X_test = all_data.loc[dates == date_block_test].drop(to_drop_cols, axis=1)\n",
    "\n",
    "shop_item2submissionid={}\n",
    "for idx, row in test_spec.iterrows():\n",
    "    shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])] = row['ID']\n",
    "    \n",
    "test_data=all_data.loc[dates == date_block_test, ['shop_id','item_id']]    \n",
    "    \n",
    "testidx2submissionidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for idx in range(test_data.shape[0]):\n",
    "    row =test_data.iloc[idx]\n",
    "    testidx2submissionidx[idx] = shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])]\n",
    "    \n",
    " \n",
    "#invert the mapping\n",
    "submissionidx2testidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for i in range(test_data.shape[0]):\n",
    "    submissionidx2testidx[testidx2submissionidx[i]]=i\n",
    "    \n",
    "del test_data\n",
    "gc.collect()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2768948,)\n",
      "(2768948, 21)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tackle ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#model=linear_model.RidgeCV(alphas=np.logspace(-3,13))\n",
    "model=linear_model.Ridge(alpha=3e6, fit_intercept=False)\n",
    "model.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(model.predict(X_val.to_numpy()), 0, 20)\n",
    "#print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "\n",
    "model.fit(X_trainval.to_numpy(), y_trainval)\n",
    "pred_test = np.clip(model.predict(X_test.to_numpy()), 0, 20)\n",
    "write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-ridge-feature_set_basic.csv')\n",
    "# LB 1.091266 and 1.088610\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#model=linear_model.RidgeCV(alphas=np.logspace(-3,13), normalize=True)\n",
    "model=linear_model.Ridge(alpha=0.2, normalize=True)\n",
    "model.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(model.predict(X_val.to_numpy()), 0, 20)\n",
    "#print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "\n",
    "model.fit(X_trainval.to_numpy(), y_trainval)\n",
    "pred_test = np.clip(model.predict(X_test.to_numpy()), 0, 20)\n",
    "write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-ridge-normalized-feature_set_basic.csv')\n",
    "# LB 1.082613 and 1.079229.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then Catboost\n",
    "from sklearn import model_selection\n",
    "\n",
    "reg=CatBoostRegressor(task_type='GPU', loss_function='RMSE', iterations=1200, eta=0.03)\n",
    "\n",
    "grid = {'depth': range(5,17)}\n",
    "        \n",
    "        \n",
    "\n",
    "reg.grid_search(grid, cv=5, X=X_train.to_numpy(), y=y_train, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.3958183\ttest: 5.3132972\tbest: 5.3132972 (0)\ttotal: 377ms\tremaining: 44.8s\n",
      "20:\tlearn: 2.3590286\ttest: 4.9282864\tbest: 4.9282864 (20)\ttotal: 6.93s\tremaining: 32.7s\n",
      "40:\tlearn: 2.1750684\ttest: 4.8845876\tbest: 4.8845876 (40)\ttotal: 13.5s\tremaining: 26.1s\n",
      "60:\tlearn: 2.0622922\ttest: 4.8864402\tbest: 4.8845876 (40)\ttotal: 20.2s\tremaining: 19.5s\n",
      "80:\tlearn: 1.9640144\ttest: 4.8881669\tbest: 4.8845876 (40)\ttotal: 26.9s\tremaining: 13s\n",
      "100:\tlearn: 1.8846092\ttest: 4.8874258\tbest: 4.8845876 (40)\ttotal: 33.6s\tremaining: 6.32s\n",
      "119:\tlearn: 1.8466087\ttest: 4.8860043\tbest: 4.8845876 (40)\ttotal: 40s\tremaining: 0us\n",
      "bestTest = 4.884587648\n",
      "bestIteration = 40\n",
      "Shrink model to first 41 iterations.\n",
      "Clipped RMSE 0.37297401685751475\n"
     ]
    }
   ],
   "source": [
    "reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=16, eta=0.3,metric_period=20)\n",
    "eval_dataset= Pool(X_val,y_val)\n",
    "reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "#print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's experiment with faster learning rate and less iterations to roughly find sensible hyperparameters\n",
    "\n",
    "# parameters to tune: depth, bootstrap_type, bagging temprerature (for Bayesian bootstrap), subsample\n",
    "# grow_policy -> min_data_in_leaf, max_leaves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict={}\n",
    "N_RAND=10\n",
    "for d in range(1,17):\n",
    "    print('depth: ',d)\n",
    "    sum = 0\n",
    "    for r in range(N_RAND):\n",
    "        reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=d, eta=0.3,metric_period=20, random_seed = r)\n",
    "        eval_dataset= Pool(X_val,y_val)\n",
    "        reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "        pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "    #print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "        print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "        sum += clipped_rmse(y_val, pred_val)\n",
    "        \n",
    "    res_dict[d] = sum / N_RAND\n",
    "  \n",
    "print(res_dict)\n",
    "\n",
    "# there's about +- 0.010 random fluctuation in scores of individual runs\n",
    "# maybe simple ensembling already on first level to avoid bad luck?\n",
    "\n",
    "\n",
    "# {1: 0.41843556427060136, 2: 0.39200723272663457, 3: 0.38961572543553347, 4: 0.39156671290508477, 5: 0.3891002776724449, 6: 0.386308048622023, 7: 0.3857013124069831, 8: 0.38184490124245063, 9: 0.3778682006343534, 10: 0.3755674367385847, 11: 0.37650542801215753, 12: 0.37621294975325814, 13: 0.37367612525383515, 14: 0.3740070625371101, 15: 0.37319345260938475, 16: 0.3703305527864274}\n",
    "# it seems that maximum available depth of 16 is a reasonable choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict={}\n",
    "N_RAND=10\n",
    "for d in range(1,6):\n",
    "    print('depth: ',d)\n",
    "    sum = 0\n",
    "    for r in range(N_RAND):\n",
    "        reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=d, eta=0.3,metric_period=20, random_seed = r)\n",
    "        eval_dataset= Pool(X_val,y_val)\n",
    "        reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "        pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "    #print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "        print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "        sum += clipped_rmse(y_val, pred_val)\n",
    "        \n",
    "    res_dict[d] = sum / N_RAND\n",
    "  \n",
    "print(res_dict)\n",
    "\n",
    "# {6: 0.37218429090880945, 7: 0.37242307494125265, 8: 0.37353756761422685, 9: 0.3725724791167392, 10: 0.37342989782446107, 11: 0.37119154253120795, 12: 0.3741465021829831, 13: 0.37239708871922106, 14: 0.3714940959972749, 15: 0.37278789161405324, 16: 0.37291028574714297}\n",
    "\n",
    "# there's no indication that a greater depth than 6 would be beneficial\n",
    "# there's about +- 0.010 random fluctuation in scores of individual runs\n",
    "# maybe simple ensembling already on first level to avoid bad luck?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict={}\n",
    "N_RAND=10\n",
    "for s in np.linspace(0.5,1.0,20):\n",
    "    print('subsample: ',s)\n",
    "    sum = 0\n",
    "    for r in range(N_RAND):\n",
    "        reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=16, eta=0.3,metric_period=20, subsample=s, bootstrap_type='Bernoulli', random_seed = r)\n",
    "        eval_dataset= Pool(X_val,y_val)\n",
    "        reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "        pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "    #print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "        print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "        sum += clipped_rmse(y_val, pred_val)\n",
    "        \n",
    "    res_dict[s] = sum / N_RAND\n",
    "  \n",
    "print(res_dict)\n",
    "\n",
    "# {0.5: 0.37720847394133683, 0.5263157894736842: 0.37968235475551115, 0.5526315789473684: 0.373719447235152, 0.5789473684210527: 0.3738004089195496, 0.6052631578947368: 0.3801445592900391, 0.631578947368421: 0.3764996905535196, 0.6578947368421053: 0.3756872315081908, 0.6842105263157895: 0.3803532474804272, 0.7105263157894737: 0.3752644353767258, 0.7368421052631579: 0.37537710048459927, 0.763157894736842: 0.37466870958854703, \n",
    "# 0.7894736842105263: 0.37254016810717994, 0.8157894736842105: 0.3770991314953297, 0.8421052631578947: 0.3736975968544275, 0.868421052631579: 0.3782591167219179, 0.8947368421052632: 0.3757776519133261, 0.9210526315789473: 0.3753221768209526, 0.9473684210526315: 0.37274794779142245, 0.9736842105263157: 0.3721892562033758, 1.0: 0.3712936972609858}\n",
    "\n",
    "# Bernoulli Sampling seems to work ok, but does not bring marked improvements\n",
    "# the value of subsample parameter seems to have little effect if at all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict={}\n",
    "N_RAND=10\n",
    "for s in np.linspace(0.5,1.0,20):\n",
    "    print('subsample: ',s)\n",
    "    sum = 0\n",
    "    for r in range(N_RAND):\n",
    "        reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=16, eta=0.3,metric_period=20, subsample=s, bootstrap_type='Poisson', random_seed = r)\n",
    "        eval_dataset= Pool(X_val,y_val)\n",
    "        reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "        pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "    #print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "        print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "        sum += clipped_rmse(y_val, pred_val)\n",
    "        \n",
    "    res_dict[s] = sum / N_RAND\n",
    "  \n",
    "print(res_dict)\n",
    "\n",
    "# roughly similar results as in 'Bernoulli' case\n",
    "# no need to use this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict={}\n",
    "N_RAND=10\n",
    "for t in np.logspace(-3,3,20):\n",
    "    print('subsample: ',s)\n",
    "    sum = 0\n",
    "    for r in range(N_RAND):\n",
    "        reg=CatBoostRegressor(task_type='GPU', iterations=120, depth=16, eta=0.3,metric_period=20, bagging_temperature=t, bootstrap_type='Bayesian', random_seed = r)\n",
    "        eval_dataset= Pool(X_val,y_val)\n",
    "        reg.fit(X_train.to_numpy(), y_train, eval_set=eval_dataset)\n",
    "        pred_val = np.clip(reg.predict(X_val.to_numpy()), 0, 20)\n",
    "    #print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "        print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "        sum += clipped_rmse(y_val, pred_val)\n",
    "        \n",
    "    res_dict[s] = sum / N_RAND\n",
    "  \n",
    "print(res_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with number of iterations: \n",
    "\n",
    "reg=CatBoostRegressor(task_type='GPU', iterations=1200, depth=16, eta=0.03,metric_period=20)\n",
    "bestTest = 4.89748216\n",
    "bestIteration = 740\n",
    "Shrink model to first 741 iterations.\n",
    "Clipped RMSE 0.3629817122046768\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4.1324599\ttotal: 356ms\tremaining: 4m 23s\n",
      "20:\tlearn: 3.5702479\ttotal: 6.81s\tremaining: 3m 53s\n",
      "40:\tlearn: 3.1942986\ttotal: 13.1s\tremaining: 3m 44s\n",
      "60:\tlearn: 2.9399711\ttotal: 19.5s\tremaining: 3m 37s\n",
      "80:\tlearn: 2.7633477\ttotal: 26s\tremaining: 3m 31s\n",
      "100:\tlearn: 2.6503653\ttotal: 32.4s\tremaining: 3m 24s\n",
      "120:\tlearn: 2.5586229\ttotal: 38.7s\tremaining: 3m 18s\n",
      "140:\tlearn: 2.4876473\ttotal: 45.2s\tremaining: 3m 11s\n",
      "160:\tlearn: 2.4200432\ttotal: 51.3s\tremaining: 3m 4s\n",
      "180:\tlearn: 2.3659357\ttotal: 57.7s\tremaining: 2m 58s\n",
      "200:\tlearn: 2.3251583\ttotal: 1m 4s\tremaining: 2m 51s\n",
      "220:\tlearn: 2.2851810\ttotal: 1m 10s\tremaining: 2m 45s\n",
      "240:\tlearn: 2.2526937\ttotal: 1m 16s\tremaining: 2m 38s\n",
      "260:\tlearn: 2.2252402\ttotal: 1m 23s\tremaining: 2m 32s\n",
      "280:\tlearn: 2.1876598\ttotal: 1m 29s\tremaining: 2m 26s\n",
      "300:\tlearn: 2.1530316\ttotal: 1m 35s\tremaining: 2m 19s\n",
      "320:\tlearn: 2.1291504\ttotal: 1m 42s\tremaining: 2m 13s\n",
      "340:\tlearn: 2.0988341\ttotal: 1m 48s\tremaining: 2m 6s\n",
      "360:\tlearn: 2.0775948\ttotal: 1m 54s\tremaining: 2m\n",
      "380:\tlearn: 2.0546454\ttotal: 2m 1s\tremaining: 1m 54s\n",
      "400:\tlearn: 2.0275536\ttotal: 2m 7s\tremaining: 1m 48s\n",
      "420:\tlearn: 2.0079614\ttotal: 2m 14s\tremaining: 1m 41s\n",
      "440:\tlearn: 1.9811735\ttotal: 2m 20s\tremaining: 1m 35s\n",
      "460:\tlearn: 1.9580515\ttotal: 2m 27s\tremaining: 1m 29s\n",
      "480:\tlearn: 1.9467850\ttotal: 2m 34s\tremaining: 1m 22s\n",
      "500:\tlearn: 1.9316384\ttotal: 2m 40s\tremaining: 1m 16s\n",
      "520:\tlearn: 1.9227921\ttotal: 2m 47s\tremaining: 1m 10s\n",
      "540:\tlearn: 1.9077181\ttotal: 2m 53s\tremaining: 1m 3s\n",
      "560:\tlearn: 1.8965323\ttotal: 3m\tremaining: 57.6s\n",
      "580:\tlearn: 1.8860106\ttotal: 3m 6s\tremaining: 51.2s\n",
      "600:\tlearn: 1.8792726\ttotal: 3m 13s\tremaining: 44.8s\n",
      "620:\tlearn: 1.8684909\ttotal: 3m 20s\tremaining: 38.3s\n",
      "640:\tlearn: 1.8629569\ttotal: 3m 26s\tremaining: 31.9s\n",
      "660:\tlearn: 1.8503239\ttotal: 3m 33s\tremaining: 25.5s\n",
      "680:\tlearn: 1.8371769\ttotal: 3m 39s\tremaining: 19.1s\n",
      "700:\tlearn: 1.8242295\ttotal: 3m 46s\tremaining: 12.6s\n",
      "720:\tlearn: 1.8172483\ttotal: 3m 53s\tremaining: 6.14s\n",
      "739:\tlearn: 1.8121703\ttotal: 3m 59s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "reg=CatBoostRegressor(task_type='GPU', iterations=740, depth=16, eta=0.03,metric_period=20)\n",
    "reg.fit(X_trainval.to_numpy(), y_trainval)\n",
    "pred_test = np.clip(reg.predict(X_test.to_numpy()), 0, 20)\n",
    "write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-catboost-feature_set_basic.csv')\n",
    "# w/ all previous months 0.989679 and 0.994916\n",
    "# w/ 12 past month examples LB 0.996330 and 0.993701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped RMSE of lgb predictions is  0.374919455274064\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':3, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':2\n",
    "              }\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb_val = np.clip(model.predict(X_val), 0, 20)\n",
    "print('Clipped RMSE of lgb predictions is ', clipped_rmse(y_val, pred_lgb_val))\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_trainval, label=y_trainval), 100)\n",
    "pred_lgb_test = np.clip(model.predict(X_test), 0, 20)\n",
    "write_predictions_by_array(pred_lgb_test[submissionidx2testidx], 'submission-lgb-feature_set_basic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost seems to give comparable performance -> use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Feature set 2: text based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(DATA_FOLDER, 'feature_set_text.csv'))\n",
    "\n",
    "dates=all_data['date_block_num']\n",
    "\n",
    "y_train = all_data.loc[(dates>= date_block_val - 9) & (dates<= date_block_val - 2), 'target']\n",
    "y_trainval = all_data.loc[(dates>= date_block_test - 9) & (dates<= date_block_test - 2), 'target']\n",
    "y_val = all_data.loc[dates == date_block_val, 'target']\n",
    "y_test = all_data.loc[dates == date_block_test, 'target']\n",
    "\n",
    "to_drop_cols = ['target','date_block_num']\n",
    "\n",
    "X_train = all_data.loc[(dates>= date_block_val - 9) & (dates<= date_block_val - 2)].drop(to_drop_cols, axis=1)\n",
    "X_trainval = all_data.loc[(dates>= date_block_test - 9) & (dates<= date_block_test - 2)].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[dates == date_block_val].drop(to_drop_cols, axis=1)\n",
    "X_test = all_data.loc[dates == date_block_test].drop(to_drop_cols, axis=1)\n",
    "\n",
    "shop_item2submissionid={}\n",
    "for idx, row in test_spec.iterrows():\n",
    "    shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])] = row['ID']\n",
    "    \n",
    "test_data=all_data.loc[dates == date_block_test, ['shop_id','item_id']]    \n",
    "    \n",
    "testidx2submissionidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for idx in range(test_data.shape[0]):\n",
    "    row =test_data.iloc[idx]\n",
    "    testidx2submissionidx[idx] = shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])]\n",
    "    \n",
    " \n",
    "#invert the mapping\n",
    "submissionidx2testidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for i in range(test_data.shape[0]):\n",
    "    submissionidx2testidx[testidx2submissionidx[i]]=i\n",
    "    \n",
    "del test_data\n",
    "gc.collect()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#model=linear_model.RidgeCV(alphas=np.logspace(-3,13), fit_intercept=True, normalize=True)\n",
    "model=linear_model.Ridge(alpha=0.04, fit_intercept=True, normalize=True)\n",
    "model.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(model.predict(X_val.to_numpy()), 0, 20)\n",
    "#print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "\n",
    "model.fit(X_trainval.to_numpy(), y_trainval)\n",
    "pred_test = np.clip(model.predict(X_test.to_numpy()), 0, 20)\n",
    "write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-ridge-feature_set_text.csv')\n",
    "# LB 1.203418 and 1.189612\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature set 3: lags within shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(DATA_FOLDER, 'feature_set_within.csv'))\n",
    "\n",
    "dates=all_data['date_block_num']\n",
    "\n",
    "y_train = all_data.loc[(dates>= date_block_val - 9) & (dates<= date_block_val - 2), 'target']\n",
    "y_trainval = all_data.loc[(dates>= date_block_test - 9) & (dates<= date_block_test - 2), 'target']\n",
    "y_val = all_data.loc[dates == date_block_val, 'target']\n",
    "y_test = all_data.loc[dates == date_block_test, 'target']\n",
    "\n",
    "to_drop_cols = ['target','date_block_num']\n",
    "\n",
    "X_train = all_data.loc[(dates>= date_block_val - 9) & (dates<= date_block_val - 2)].drop(to_drop_cols, axis=1)\n",
    "X_trainval = all_data.loc[(dates>= date_block_test - 9) & (dates<= date_block_test - 2)].drop(to_drop_cols, axis=1)\n",
    "X_val = all_data.loc[dates == date_block_val].drop(to_drop_cols, axis=1)\n",
    "X_test = all_data.loc[dates == date_block_test].drop(to_drop_cols, axis=1)\n",
    "\n",
    "shop_item2submissionid={}\n",
    "for idx, row in test_spec.iterrows():\n",
    "    shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])] = row['ID']\n",
    "    \n",
    "test_data=all_data.loc[dates == date_block_test, ['shop_id','item_id']]    \n",
    "    \n",
    "testidx2submissionidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for idx in range(test_data.shape[0]):\n",
    "    row =test_data.iloc[idx]\n",
    "    testidx2submissionidx[idx] = shop_item2submissionid[str(row['shop_id'])+'_'+str(row['item_id'])]\n",
    "    \n",
    " \n",
    "#invert the mapping\n",
    "submissionidx2testidx=np.zeros(test_data.shape[0], dtype=np.int32)\n",
    "for i in range(test_data.shape[0]):\n",
    "    submissionidx2testidx[testidx2submissionidx[i]]=i\n",
    "    \n",
    "del test_data\n",
    "gc.collect()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#model=linear_model.RidgeCV(alphas=np.logspace(-3,13), fit_intercept=False)\n",
    "model=linear_model.Ridge(alpha=3e7, fit_intercept=False)\n",
    "model.fit(X_train.to_numpy(), y_train)\n",
    "pred_val = np.clip(model.predict(X_val.to_numpy()), 0, 20)\n",
    "#print('Validation R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb_val))\n",
    "print('Clipped RMSE {}'.format(clipped_rmse(y_val, pred_val)))\n",
    "\n",
    "model.fit(X_trainval.to_numpy(), y_trainval)\n",
    "pred_test = np.clip(model.predict(X_test.to_numpy()), 0, 20)\n",
    "write_predictions_by_array(pred_test[submissionidx2testidx], 'submission-ridge-feature_set_within.csv')\n",
    "# LB 1.215079 and 1.202396\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Kaggle-C1-text-features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "22170b496ee045b3b9975d05b0a4d8e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47f2f9b1fadf44e7829b7a5dd04f994d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56c59abdf53e4ccfae147a1b1e8408eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76ab44c3980e4b7c8d552d0a53195975",
       "IPY_MODEL_7f9d57175f6149d08092c23799891b34"
      ],
      "layout": "IPY_MODEL_f57a7b904c3d4c2ab82802e25b0b1b21"
     }
    },
    "76ab44c3980e4b7c8d552d0a53195975": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e308dc6f5047435b95ca5e6d036b2450",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2adb6d1bead4670ae9ce7c3e32590c7",
      "value": 4
     }
    },
    "7f9d57175f6149d08092c23799891b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47f2f9b1fadf44e7829b7a5dd04f994d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_22170b496ee045b3b9975d05b0a4d8e7",
      "value": "100% 4/4 [00:45&lt;00:00, 11.42s/it]"
     }
    },
    "e308dc6f5047435b95ca5e6d036b2450": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2adb6d1bead4670ae9ce7c3e32590c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f57a7b904c3d4c2ab82802e25b0b1b21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
